{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67ff21b3-3ef5-41f9-acf4-8c06c18b81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_path = \"prices.txt\"\n",
    "products_path = \"products.txt\"\n",
    "sales_path = \"sales2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be769f7-927e-4910-8293-bb945ba67585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fd9b4d4-91b3-423e-a103-e40f8427ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2021', ('pid2', 4)), ('2021', ('pid3', 4)), ('2020', ('pid2', 4)), ('2020', ('pid3', 4)), ('2019', ('pid2', 4)), ('2019', ('pid3', 4)), ('2018', ('pid2', 4)), ('2018', ('pid3', 4)), ('2021', ('pid1', 4)), ('2020', ('pid1', 4))]\n",
      "[('2021', 'pid2'), ('2021', 'pid3'), ('2021', 'pid1'), ('2020', 'pid2'), ('2020', 'pid3'), ('2020', 'pid1'), ('2019', 'pid2'), ('2019', 'pid3'), ('2019', 'pid1'), ('2018', 'pid2')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def modify_sales_date(row):\n",
    "    date = row.split(\",\")[1]\n",
    "    year = date.split(\"/\")[0]\n",
    "\n",
    "    new_row = row.split(\",\")[0] + \",\" + year + \",\" + row.split(\",\")[2]\n",
    "    return new_row\n",
    "\n",
    "def make_key_value_pairs(row):\n",
    "    return((row.split(\",\")[0], row.split(\",\")[1]), row.split(\",\")[2])\n",
    "\n",
    "def modify_key_value_pair(row):\n",
    "    return (row[0][0], (row[0][1], row[1]))\n",
    "\n",
    "def filter_condition(row):\n",
    "    values = row[1]\n",
    "    if len(values) > 1:\n",
    "        pair1 = values[0]\n",
    "        year1 = pair1[0]\n",
    "        total1 = pair1[1]\n",
    "\n",
    "        pair2 = values[1]\n",
    "        year2 = pair2[0]\n",
    "        total2 = pair2[1]\n",
    "\n",
    "        if year1 == '2021':\n",
    "            if total1 < total2:\n",
    "                return True\n",
    "        else:\n",
    "            if total2 < total1:\n",
    "                return True\n",
    "        return False\n",
    "    else:\n",
    "        pair1 = values[0]\n",
    "        year1 = pair1[0]\n",
    "        if year1 == '2019':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def keep_max_prods(row):\n",
    "    values = row[1]\n",
    "    max_val = 0\n",
    "    for v in values:\n",
    "        if v[1] > max_val:\n",
    "            max_val = v[1]\n",
    "    new_values = [v for v in values if v[1] == max_val]\n",
    "    new_row = [(row[0], v[0]) for v in new_values]    \n",
    "    return new_row\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf().setAppName(\"lab 7\")\n",
    "    ctx = SparkContext.getOrCreate(conf)\n",
    "\n",
    "    # Ex1\n",
    "    sales_RDD = ctx.textFile(sales_path)\n",
    "    sales_RDD = sales_RDD.map(modify_sales_date)\n",
    "    # KEEP only 2019 and 2021\n",
    "    filtered_sales_RDD = sales_RDD.filter(lambda row: row.split(\",\")[1] == '2019' or row.split(\",\")[1] == '2021')\n",
    "    filtered_sales_RDD = filtered_sales_RDD.map(make_key_value_pairs)\n",
    "\n",
    "    reduced_sales = filtered_sales_RDD.reduceByKey(lambda v1, v2: int(v1) + int(v2))\n",
    "    reduced_sales = reduced_sales.map(modify_key_value_pair)\n",
    "    # reduced_sales = reduced_sales.reduceByKey()\n",
    "    reduced_sales = reduced_sales.groupByKey().mapValues(lambda values: list(values))\n",
    "    filtered_RDD = reduced_sales.filter(filter_condition)\n",
    "\n",
    "    final_RDD_ex1 = filtered_RDD.map(lambda x: x[0]) \n",
    "    final_RDD_ex1.saveAsTextFile(\"ex1\")\n",
    "\n",
    "    # EX2\n",
    "    sales_RDD = ctx.textFile(sales_path)\n",
    "    # Make key-value pairs\n",
    "    sales_RDD = sales_RDD.map(lambda row: ((row.split(\",\")[0], row.split(\",\")[1].split(\"/\")[0]), 1))\n",
    "    reduced_RDD = sales_RDD.reduceByKey(lambda v1, v2: v1+v2)\n",
    "\n",
    "    rdd_year_key = reduced_RDD.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "    print(rdd_year_key.take(10))\n",
    "    grouped_rdd = rdd_year_key.groupByKey().mapValues(lambda v: list(v))\n",
    "    max_prod_for_each_year = grouped_rdd.flatMap(keep_max_prods)\n",
    "\n",
    "    max_prod_for_each_year.saveAsTextFile(\"ex2\")\n",
    "    print(max_prod_for_each_year.take(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "kernel_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
